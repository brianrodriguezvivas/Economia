{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "class Filtrado:\n",
    "    def __init__(self,hora_inicial=6, hora_final=17):\n",
    "        # self.nombre_analizador = Analizador\n",
    "        self.hora_inicial = hora_inicial\n",
    "        self.hora_final = hora_final   \n",
    "    \n",
    "    def Archivos(self,Ruta):\n",
    "        \"\"\"\n",
    "        Esta función recibe una ruta y retorna una lista de rutas completas de archivos en el directorio especificado, excluyendo '.DS_Store' si está presente.\n",
    "\n",
    "        Args:\n",
    "            Ruta (str): Ruta del directorio.\n",
    "\n",
    "        Returns:\n",
    "            list: Lista de rutas completas de archivos en el directorio.\n",
    "        \"\"\"\n",
    "        archivos = [archivo for archivo in os.listdir(Ruta) if not archivo.startswith('.DS_Store')]\n",
    "        rutas_completas = [os.path.join(Ruta, archivo) for archivo in archivos]\n",
    "        return rutas_completas\n",
    "\n",
    "    def Lectura(self,list_rutas,):\n",
    "        \"\"\"\n",
    "        Lee un archivo CSV o Excel y devuelve un DataFrame.\n",
    "\n",
    "        Parámetros:\n",
    "        archivo (str): Ruta del archivo a leer.\n",
    "\n",
    "        Retorna:\n",
    "        DataFrame: DataFrame con los datos del archivo leído.\n",
    "\n",
    "        Errores:\n",
    "        ValueError: Si el formato del archivo no es compatible.\n",
    "        IOError: Si ocurre un error al leer el archivo.\n",
    "        \"\"\"\n",
    "        list_df = []\n",
    "        for ruta in list_rutas:\n",
    "            try:\n",
    "                if ruta.endswith(\".csv\"):\n",
    "                    df = pd.read_csv(ruta )\n",
    "                elif ruta.endswith(\".xlsx\") or ruta.endswith(\".xls\"):\n",
    "                    \n",
    "                    dataframes_por_hoja = pd.read_excel(ruta, sheet_name=None)\n",
    "                    list_df.append(pd.concat(dataframes_por_hoja.values(), ignore_index=True))\n",
    "                else:\n",
    "                    raise ValueError(\"Formato de archivo no compatible.\")\n",
    "                \n",
    "            except pd.errors.EmptyDataError:\n",
    "                return {\"error\": \"El archivo está vacío.\"}\n",
    "            except IOError as e:\n",
    "                return {\"error\": f\"Error al leer el archivo '{ruta}': {e}\"}\n",
    "        return pd.concat(list_df, ignore_index=True)\n",
    "    \n",
    "    def Arreglo(self, df,nombre_analizador):\n",
    "        \"\"\"\n",
    "        Filtra y procesa un DataFrame según los criterios establecidos.\n",
    "        Borrar elementos duplicados y filtrar por analizador y rango de horas.\n",
    "\n",
    "        Parámetros:\n",
    "        - df: DataFrame a filtrar y procesar.\n",
    "\n",
    "        Retorna:\n",
    "        - df_filtrado: DataFrame filtrado y procesado.\n",
    "        \"\"\"\n",
    "        df = df[df['analizador'] == nombre_analizador].reset_index(drop=True)\n",
    "        df[\"datetime\"] = pd.to_datetime(df['datetime'])\n",
    "        df.insert(1, \"Fechas\", pd.to_datetime(df[\"datetime\"].dt.strftime('%Y-%m-%d %H:%M')))\n",
    "        del df[\"datetime\"]\n",
    "        df2 = df.drop_duplicates(subset=['Fechas'])\n",
    "        df_filtrado = df2[df2['Fechas'].dt.hour.between(self.hora_inicial, self.hora_final)]\n",
    "        df_filtrado = df_filtrado.reset_index(drop=True)\n",
    "        return df_filtrado\n",
    "    \n",
    "    def Series_tiempo(self, df):\n",
    "        \"\"\"\n",
    "        Calcula la serie de tiempo promedio de la columna 'Pmax' en base a los valores de fecha y hora del DataFrame proporcionado.\n",
    "\n",
    "        Parámetros:\n",
    "        - df: DataFrame que contiene la columna 'Fechas' y 'Pmax'.\n",
    "\n",
    "        Retorna:\n",
    "        - numpy.array: Un arreglo numpy que contiene los valores promedio de 'Pmax' agrupados por mes, hora y minuto repetidos por mes\n",
    "        \"\"\"\n",
    "        df[\"Mes\"] = df[\"Fechas\"].dt.month\n",
    "        df[\"Hora\"] = df[\"Fechas\"].dt.hour\n",
    "        df[\"Minuto\"] = df[\"Fechas\"].dt.minute\n",
    "        lista = []\n",
    "        df_filtrado_minutal_prom = df.groupby([\"Mes\", \"Hora\", \"Minuto\"])['Pmax'].mean()\n",
    "        for i in range(len(df[\"Fechas\"])):\n",
    "            lista.append(df_filtrado_minutal_prom[(df[\"Fechas\"][i].month, df[\"Fechas\"][i].hour, df[\"Fechas\"][i].minute)],)\n",
    "        \n",
    "        return np.array(lista)\n",
    "    \n",
    "    def Procesamiento(self, df,nombre_analizador):\n",
    "        \"\"\"\n",
    "        Realiza el procesamiento de un DataFrame dado.\n",
    "        Le agrega las fechas faltantes de valores faltantes en la columna 'Pmax' de las horas de nuestro interes.\n",
    "\n",
    "        Parameters:\n",
    "        - df: DataFrame\n",
    "            El DataFrame que se desea procesar.\n",
    "\n",
    "        Returns:\n",
    "        - df_filtrado: DataFrame\n",
    "            El DataFrame procesado.\n",
    "        \"\"\"\n",
    "        df_filtrado = self.Arreglo(df,nombre_analizador)\n",
    "        U = pd.Timestamp(year=df_filtrado[\"Fechas\"].iloc[0].year, month=1, day=1, hour=6)\n",
    "\n",
    "        fechas_unir = pd.date_range(start=U, end=df_filtrado[\"Fechas\"].iloc[-1], freq='1min')\n",
    "        data_fechas = pd.DataFrame(fechas_unir, columns=[\"Fechas\"])\n",
    "        data_fechas2 = data_fechas.loc[data_fechas['Fechas'].dt.hour.between(self.hora_inicial, self.hora_final)]\n",
    "        df_filtrado = pd.merge(df_filtrado, data_fechas2, on='Fechas', how='outer')\n",
    "        df_filtrado = df_filtrado.sort_values(by='Fechas').reset_index(drop=True)\n",
    "\n",
    "        return df_filtrado\n",
    "\n",
    "    \n",
    "    def imputacion_1_2(self, df, Serie_media_minutal_prom):\n",
    "        \"\"\"\n",
    "        Esta función realiza la imputación de valores faltantes en el dataframe 'df' utilizando \n",
    "        vecinos cercanos y factor de degradación.\n",
    "        \n",
    "        Parámetros:\n",
    "        - df: DataFrame: El dataframe que contiene los datos a imputar.\n",
    "        - Serie_media_minutal_prom: Serie: La serie de valores medios por minuto utilizada para la imputación.\n",
    "        \n",
    "        Retorna:\n",
    "        - DataFrame: El dataframe 'df' con los valores faltantes imputados.\n",
    "        \"\"\"\n",
    "        df = df.sort_values(by='Fechas').reset_index(drop=True)\n",
    "        indices_filas = []\n",
    "        \n",
    "        \n",
    "        def prueba(df, i, count):\n",
    "            \"\"\"\n",
    "            Esta función realiza la imputación de valores faltantes en el dataframe 'df' utilizando\n",
    "            factor de degradación o remplazando por la serie media de los valores faltantes.\n",
    "            \n",
    "            Parámetros:\n",
    "            - df: DataFrame: El dataframe que contiene los datos a imputar.\n",
    "            - i: int: El índice actual en el bucle.\n",
    "            - count: int: El contador de valores faltantes consecutivos.\n",
    "            - valor: int: Indicador de si se debe imputar con factor de degradación o serie media.\n",
    "                0: Serie media y caso contrario Factor de degradación.\n",
    "            \n",
    "            \n",
    "            Retorna:\n",
    "            - DataFrame: El dataframe 'df' con los valores faltantes imputados.\n",
    "            \"\"\"\n",
    "            Valor_Izq = df.loc[i-count-1,\"Pmax\"]\n",
    "            Valor_Der = df.loc[i, \"Pmax\"]\n",
    "            \n",
    "            Serie_media_Izq = Serie_media_minutal_prom[i-count-1]\n",
    "            Serie_media_Der = Serie_media_minutal_prom[i]\n",
    "        \n",
    "            \n",
    "            if Serie_media_Izq == 0 or np.isnan(Valor_Izq / Serie_media_Izq) :\n",
    "                FactIni = 0\n",
    "            else: \n",
    "                FactIni = Valor_Izq / Serie_media_Izq\n",
    "\n",
    "            # Verificar si la división es NaN para Valor_Der/Serie_media_Der\n",
    "            if Serie_media_Der == 0 or np.isnan(Valor_Der / Serie_media_Der)  :\n",
    "                FactFin = 0\n",
    "            else:\n",
    "                FactFin = Valor_Der / Serie_media_Der\n",
    "\n",
    "            Step = abs((FactFin-FactIni) / count)\n",
    "            Dir = 1 if (FactFin- FactIni) >= 0 else -1\n",
    "            \n",
    "            FactDegra = [FactIni + Dir * k * Step for k in range(1, count+1)]\n",
    "\n",
    "            return Serie_media_minutal_prom[i-count:i ]*np.array(FactDegra)\n",
    "        \n",
    "        \n",
    "        count = 0\n",
    "        for i in range(len(df[\"Fechas\"])):\n",
    "            \n",
    "            if pd.isna(df.loc[i, \"Pmax\"]):\n",
    "                count += 1\n",
    "                \n",
    "                \n",
    "            elif (i != 0) and (pd.isna(df.loc[i-1,'Pmax'])) and (~pd.isna(df.loc[i,'Pmax'])):\n",
    "\n",
    "                if count == 1:\n",
    "                    df.loc[i-1, \"Pmax\"] = (df.loc[i-2, \"Pmax\"] + df.loc[i, \"Pmax\"]) / 2\n",
    "                    count = 0\n",
    "                elif count == 2:\n",
    "                    df.loc[i-2, \"Pmax\"] = (df.loc[i-3, \"Pmax\"] + df.loc[i, \"Pmax\"]) / 2\n",
    "                    df.loc[i-1, \"Pmax\"] = (df.loc[i-2, \"Pmax\"] + df.loc[i, \"Pmax\"]) / 2                   \n",
    "                    count = 0\n",
    "                elif count > 2 and count <720:\n",
    "                    F = np.arange(i-count,i)\n",
    "                    df.loc[F, \"Pmax\"] = prueba(df, i, count)\n",
    "                    \n",
    "                    count = 0                \n",
    "                elif count >= 720:\n",
    "                    F = np.arange(i-count,i)\n",
    "\n",
    "                    df.loc[F,'Pmax'] = Serie_media_minutal_prom[i-count:i]\n",
    "                    count =0\n",
    "                    \n",
    "        if pd.isna(df.loc[i,'Pmax']):\n",
    "            if count == 1:\n",
    "                df.loc[i, \"Pmax\"] = (df.loc[i-2, \"Pmax\"] + df.loc[i-1, \"Pmax\"]) / 2\n",
    "                count = 0\n",
    "            elif count == 2:\n",
    "                df.loc[i-1, \"Pmax\"] = (df.loc[i-2, \"Pmax\"] + df.loc[i-3, \"Pmax\"]) / 2\n",
    "                df.loc[i, \"Pmax\"] = (df.loc[i-1, \"Pmax\"] + df.loc[i-2, \"Pmax\"]) / 2                   \n",
    "                count = 0\n",
    "            elif count > 2 and count <720:\n",
    "                F = np.arange(i-count,i)\n",
    "                df.loc[F, \"Pmax\"] = prueba(df, i, count)\n",
    "                \n",
    "                count = 0                \n",
    "            elif count >= 720:\n",
    "                # print(i+1, \" i+1\")\n",
    "                # print(count, \" count\")\n",
    "                # print(i-count, \" i-count\")\n",
    "                F = np.arange(i-count,i+1)\n",
    "                # print(len(df.loc[F,'Pmax']),\"F_Pmax\")\n",
    "                # print(len(Serie_media_minutal_prom[i-count:i+1]),\" Serie\")\n",
    "                df.loc[F,'Pmax'] = Serie_media_minutal_prom[i-count:i+1]\n",
    "                count =0\n",
    "                    \n",
    "                \n",
    "        \n",
    "        # print(count, \" count final \")\n",
    "\n",
    "        return df\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analizadores = [\"AnP01_\",\"AnP03_\", \"AnP04_\",\"AnP10_\"]\n",
    "\n",
    "# ruta_para_guardar = \"/Users/brianrodriguez/Documents/Economia/Datos_nueva_serie\"\n",
    "\n",
    "ruta_para_guardar = \"/Users/brianrodriguez/Documents/Economia/Data\"\n",
    "year = [2021,2022,2023]\n",
    "def Crear_excel(df, nombre_analizador, año, ruta):\n",
    "    with pd.ExcelWriter(f'{ruta}/{nombre_analizador}Sol_PV_{año}.xlsx', engine='openpyxl') as writer:\n",
    "        df.rename(columns={'Fechas': 'datetime'}, inplace=True)\n",
    "        for month, data in df.groupby(df['datetime'].dt.month):\n",
    "            month_name = data['datetime'].dt.strftime('%B').iloc[0]\n",
    "            data[['datetime', 'Pmax']].to_excel(writer, sheet_name=month_name, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ruta = os.getcwd()+\"/Prueba_data\"\n",
    "ASD = Filtrado( )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = ASD.Archivos(ruta)\n",
    "data = ASD.Lectura(lista)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Para serie promedio entre 3 años #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for q in Analizadores:\n",
    "\n",
    "    data_filtrado = ASD.Procesamiento(data,q)\n",
    "    data_filtrado_21= data_filtrado[data_filtrado[\"Fechas\"].dt.year == 2021].reset_index(drop=True)\n",
    "    data_filtrado_22= data_filtrado[data_filtrado[\"Fechas\"].dt.year == 2022].reset_index(drop=True)\n",
    "    data_filtrado_23= data_filtrado[data_filtrado[\"Fechas\"].dt.year == 2023].reset_index(drop=True)\n",
    "    serie_21 = ASD.Series_tiempo(data_filtrado_21)\n",
    "    serie_22 = ASD.Series_tiempo(data_filtrado_22)\n",
    "    serie_23 = ASD.Series_tiempo(data_filtrado_23)\n",
    "    V = np.array([serie_21,serie_22,serie_23])\n",
    "    Serie_total = np.nanmean(V, axis=0)\n",
    "    G1 = ASD.imputacion_1_2(data_filtrado_21, Serie_total)\n",
    "    G2 = ASD.imputacion_1_2(data_filtrado_22, Serie_total)\n",
    "    G3 = ASD.imputacion_1_2(data_filtrado_23, Serie_total)\n",
    "    Crear_excel(G1,q ,2021, ruta_para_guardar)\n",
    "    Crear_excel(G2,q ,2022, ruta_para_guardar)\n",
    "    Crear_excel(G3,q ,2023, ruta_para_guardar)\n",
    "\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Para imputación Alejandro #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for q in Analizadores:\n",
    "\n",
    "    data_filtrado = ASD.Procesamiento(data,q)\n",
    "    data_filtrado_21= data_filtrado[data_filtrado[\"Fechas\"].dt.year == 2021].reset_index(drop=True)\n",
    "    data_filtrado_22= data_filtrado[data_filtrado[\"Fechas\"].dt.year == 2022].reset_index(drop=True)\n",
    "    data_filtrado_23= data_filtrado[data_filtrado[\"Fechas\"].dt.year == 2023].reset_index(drop=True)\n",
    "    serie_22 = ASD.Series_tiempo(data_filtrado_22)\n",
    "    serie_23 = ASD.Series_tiempo(data_filtrado_23)\n",
    "    serie_21 = (serie_22 + serie_23)/2\n",
    "    Serie_total = np.nanmean(V, axis=0)\n",
    "    G1 = ASD.imputacion_1_2(data_filtrado_21, serie_21)\n",
    "    G2 = ASD.imputacion_1_2(data_filtrado_22, serie_22)\n",
    "    G3 = ASD.imputacion_1_2(data_filtrado_23, serie_23)\n",
    "    Crear_excel(G1,q ,2021, ruta_para_guardar)\n",
    "    Crear_excel(G2,q ,2022, ruta_para_guardar)\n",
    "    Crear_excel(G3,q ,2023, ruta_para_guardar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "economia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
